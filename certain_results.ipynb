{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7243dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_model\n",
    "from run_model import run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ee2f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"domain\" # choose checkpoint type\n",
    "model_checkpoints = {\n",
    "    \"fast\": \"distilbert-base-uncased\",\n",
    "    \"base\": \"bert-base-uncased\",\n",
    "    \"domain\": \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f86c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning data preprocessing\n",
      "Processed data written to certainty_tokens200_train_for_token_classification.json and certainty_tokens200_dev_for_token_classification.json\n",
      "Train label distribution:\n",
      "Certain: 932\n",
      "Hypothetical: 103\n",
      "Conditional: 81\n",
      "Dev label distribution:\n",
      "Certain: 157\n",
      "Hypothetical: 27\n",
      "Conditional: 15\n",
      "Loading dataset into huggingface format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-5755d26b9959350b\n",
      "Reusing dataset json (/home/brentdevries/.cache/huggingface/datasets/json/default-5755d26b9959350b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d4ad61ae85f48b09a1bdd06606a30cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets using BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/brentdevries/.cache/huggingface/datasets/json/default-5755d26b9959350b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-22c457323a927766.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f625755f8d84e9dbadae9772421fe9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT token classification model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running training *****\n",
      "  Num examples = 541\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 3400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sent to cuda\n",
      "Setting up training configuration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3400' max='3400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3400/3400 1:05:30, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Micro Precision</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Micro Recall</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.161068</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045685</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.080357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.156636</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045685</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.080357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.149548</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045685</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.080357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.140072</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045685</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.080357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.129025</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045685</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.080357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.116231</td>\n",
       "      <td>0.141414</td>\n",
       "      <td>0.141414</td>\n",
       "      <td>0.379252</td>\n",
       "      <td>0.141414</td>\n",
       "      <td>0.335470</td>\n",
       "      <td>0.141414</td>\n",
       "      <td>0.084964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.101309</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.267836</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.329535</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.096338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.086757</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>0.310506</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>0.340693</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>0.139245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.072659</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.381857</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.363865</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>0.211721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.058629</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.386953</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.365290</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.256927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.046191</td>\n",
       "      <td>0.414141</td>\n",
       "      <td>0.414141</td>\n",
       "      <td>0.395690</td>\n",
       "      <td>0.414141</td>\n",
       "      <td>0.379060</td>\n",
       "      <td>0.414141</td>\n",
       "      <td>0.298017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.035276</td>\n",
       "      <td>0.489899</td>\n",
       "      <td>0.489899</td>\n",
       "      <td>0.304354</td>\n",
       "      <td>0.489899</td>\n",
       "      <td>0.339981</td>\n",
       "      <td>0.489899</td>\n",
       "      <td>0.286550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.026868</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.291597</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.314103</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.285017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.017527</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.301987</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.327873</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.308454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>1.011921</td>\n",
       "      <td>0.671717</td>\n",
       "      <td>0.671717</td>\n",
       "      <td>0.316872</td>\n",
       "      <td>0.671717</td>\n",
       "      <td>0.345442</td>\n",
       "      <td>0.671717</td>\n",
       "      <td>0.329739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>1.006909</td>\n",
       "      <td>0.691919</td>\n",
       "      <td>0.691919</td>\n",
       "      <td>0.317826</td>\n",
       "      <td>0.691919</td>\n",
       "      <td>0.343780</td>\n",
       "      <td>0.691919</td>\n",
       "      <td>0.330293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>1.002740</td>\n",
       "      <td>0.712121</td>\n",
       "      <td>0.712121</td>\n",
       "      <td>0.328709</td>\n",
       "      <td>0.712121</td>\n",
       "      <td>0.352327</td>\n",
       "      <td>0.712121</td>\n",
       "      <td>0.339685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>1.000083</td>\n",
       "      <td>0.717172</td>\n",
       "      <td>0.717172</td>\n",
       "      <td>0.321970</td>\n",
       "      <td>0.717172</td>\n",
       "      <td>0.344255</td>\n",
       "      <td>0.717172</td>\n",
       "      <td>0.331530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>0.996048</td>\n",
       "      <td>0.737374</td>\n",
       "      <td>0.737374</td>\n",
       "      <td>0.308489</td>\n",
       "      <td>0.737374</td>\n",
       "      <td>0.332384</td>\n",
       "      <td>0.737374</td>\n",
       "      <td>0.314873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>0.993623</td>\n",
       "      <td>0.742424</td>\n",
       "      <td>0.742424</td>\n",
       "      <td>0.312543</td>\n",
       "      <td>0.742424</td>\n",
       "      <td>0.334520</td>\n",
       "      <td>0.742424</td>\n",
       "      <td>0.316813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>0.990426</td>\n",
       "      <td>0.752525</td>\n",
       "      <td>0.752525</td>\n",
       "      <td>0.298060</td>\n",
       "      <td>0.752525</td>\n",
       "      <td>0.328585</td>\n",
       "      <td>0.752525</td>\n",
       "      <td>0.304509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>0.986315</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.329078</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.340931</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.322858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>0.982265</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.260417</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.320513</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.287356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>0.979060</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.346491</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.345204</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.327113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>0.975347</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.317708</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.334995</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.309474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>0.970819</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.317708</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.334995</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.309474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>0.966135</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.364184</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.355413</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.342814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>0.960208</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.364184</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.355413</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.342814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.133800</td>\n",
       "      <td>0.954384</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.376344</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.365622</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.356875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>0.947596</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.376344</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.365622</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.356875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>0.940484</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.385352</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.375831</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.369536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>0.933001</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.385352</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.375831</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.369536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>0.923982</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.415064</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.400522</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.398468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>0.914463</td>\n",
       "      <td>0.782828</td>\n",
       "      <td>0.782828</td>\n",
       "      <td>0.429776</td>\n",
       "      <td>0.782828</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.782828</td>\n",
       "      <td>0.420982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>0.903187</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.449182</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.437559</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.437465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>0.894364</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.458928</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.437559</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.440685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>0.881457</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.458928</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.437559</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.440685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>0.869416</td>\n",
       "      <td>0.782828</td>\n",
       "      <td>0.782828</td>\n",
       "      <td>0.448508</td>\n",
       "      <td>0.782828</td>\n",
       "      <td>0.433286</td>\n",
       "      <td>0.782828</td>\n",
       "      <td>0.435218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>0.856092</td>\n",
       "      <td>0.782828</td>\n",
       "      <td>0.782828</td>\n",
       "      <td>0.602890</td>\n",
       "      <td>0.782828</td>\n",
       "      <td>0.483666</td>\n",
       "      <td>0.782828</td>\n",
       "      <td>0.506343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>0.844603</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.580576</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.459307</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.464249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>0.831774</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.586124</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.461443</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.467969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>0.822503</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.608718</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.498148</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.517355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>0.813218</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.606569</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.508357</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.521747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.960100</td>\n",
       "      <td>0.798812</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.597696</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.506220</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.515972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.792363</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.600940</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.506220</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.517772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.779103</td>\n",
       "      <td>0.782828</td>\n",
       "      <td>0.782828</td>\n",
       "      <td>0.591405</td>\n",
       "      <td>0.782828</td>\n",
       "      <td>0.514292</td>\n",
       "      <td>0.782828</td>\n",
       "      <td>0.514893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.768949</td>\n",
       "      <td>0.762626</td>\n",
       "      <td>0.762626</td>\n",
       "      <td>0.575541</td>\n",
       "      <td>0.762626</td>\n",
       "      <td>0.505745</td>\n",
       "      <td>0.762626</td>\n",
       "      <td>0.499802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.758555</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.634942</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.550190</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.558495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.758332</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.584345</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.512156</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.508637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.744396</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.595320</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.550855</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.523774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.748639</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.658120</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.541121</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.534836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.735405</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.615840</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.563675</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.546538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.724263</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.608402</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.567474</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.539744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.714218</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.651731</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.593970</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.579427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.713480</td>\n",
       "      <td>0.823232</td>\n",
       "      <td>0.823232</td>\n",
       "      <td>0.669619</td>\n",
       "      <td>0.823232</td>\n",
       "      <td>0.612726</td>\n",
       "      <td>0.823232</td>\n",
       "      <td>0.599109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.700376</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.665573</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.610589</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.595083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.696070</td>\n",
       "      <td>0.813131</td>\n",
       "      <td>0.813131</td>\n",
       "      <td>0.661705</td>\n",
       "      <td>0.813131</td>\n",
       "      <td>0.608452</td>\n",
       "      <td>0.813131</td>\n",
       "      <td>0.591130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.693411</td>\n",
       "      <td>0.823232</td>\n",
       "      <td>0.823232</td>\n",
       "      <td>0.669619</td>\n",
       "      <td>0.823232</td>\n",
       "      <td>0.612726</td>\n",
       "      <td>0.823232</td>\n",
       "      <td>0.599109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.680455</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.717961</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.669516</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.660425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.694241</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.757842</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.655840</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.655132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.700296</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.744040</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.635755</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.630477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.666920</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.716201</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.669516</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.658533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.690254</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.741137</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.680199</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.682093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.685905</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.724383</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.673789</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.666766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.685307</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.706140</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.675926</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.669836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.673896</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.706140</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.675926</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.669836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.671814</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.698889</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.673789</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.663251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.708788</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.731179</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.675926</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.673160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.725029</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.706140</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.675926</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.669836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.742802</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.701286</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.673789</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.665409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.805044</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.744152</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.669991</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.681064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.796169</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.714241</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.667854</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.673336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.797168</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.708487</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.665717</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.668663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.796902</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.714466</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.678063</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.676827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.873498</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.744368</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.680199</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.684563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.881411</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.714241</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.667854</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.673336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.877508</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.720161</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.680199</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.681559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.880401</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.711698</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.687939</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.690904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.943404</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.747403</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.694349</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.708654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.972753</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.740534</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.692213</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.703624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>1.043449</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.778472</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.694349</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.717958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>1.008615</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.750951</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.704558</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.714924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>1.025069</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.750951</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.704558</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.714924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>1.059060</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.730433</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.682336</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.689230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>1.069949</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.750951</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.704558</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.714924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>1.095371</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.711698</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.687939</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.690904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>1.139378</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.756046</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.704558</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.718025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>1.152174</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.750951</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.704558</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.714924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>1.159211</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.749893</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.726781</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.732832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>1.189007</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.742199</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.712298</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.722244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>1.207211</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.728129</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.690076</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.701774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>1.226564</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.736806</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.672127</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.688972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>1.264052</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.744637</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.674264</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.694274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>1.258436</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.735965</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.692213</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.707090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>1.308043</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.756046</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.704558</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.718025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>1.315537</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.728579</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.702422</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.707117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>1.315980</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.755433</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.726781</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.736070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>1.432746</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.766736</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.674264</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.696685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>1.388288</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.758357</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.694349</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.714888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>1.367904</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.741350</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.704558</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.715593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to cmed-certainty/checkpoint-500\n",
      "Configuration saved in cmed-certainty/checkpoint-500/config.json\n",
      "Model weights saved in cmed-certainty/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-certainty/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in cmed-certainty/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to cmed-certainty/checkpoint-1000\n",
      "Configuration saved in cmed-certainty/checkpoint-1000/config.json\n",
      "Model weights saved in cmed-certainty/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-certainty/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in cmed-certainty/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-certainty/checkpoint-1500\n",
      "Configuration saved in cmed-certainty/checkpoint-1500/config.json\n",
      "Model weights saved in cmed-certainty/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-certainty/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in cmed-certainty/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-certainty/checkpoint-2000\n",
      "Configuration saved in cmed-certainty/checkpoint-2000/config.json\n",
      "Model weights saved in cmed-certainty/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-certainty/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in cmed-certainty/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-certainty/checkpoint-2500\n",
      "Configuration saved in cmed-certainty/checkpoint-2500/config.json\n",
      "Model weights saved in cmed-certainty/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-certainty/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in cmed-certainty/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-certainty/checkpoint-3000\n",
      "Configuration saved in cmed-certainty/checkpoint-3000/config.json\n",
      "Model weights saved in cmed-certainty/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-certainty/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in cmed-certainty/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: certainty_tags, tokens.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 88\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_model(\"certainty\", model_checkpoints[model_type], epochs=100, classification=\"token\", chunk_by=\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d00de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning data preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ffb67a2f790de55e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data written to certainty_sentence_train_for_sequence_classification.json and certainty_sentence_dev_for_sequence_classification.json\n",
      "Train label distribution:\n",
      "Certain: 932\n",
      "Hypothetical: 103\n",
      "Conditional: 81\n",
      "Dev label distribution:\n",
      "Certain: 157\n",
      "Hypothetical: 27\n",
      "Conditional: 15\n",
      "Loading dataset into huggingface format\n",
      "Downloading and preparing dataset json/default to /home/brentdevries/.cache/huggingface/datasets/json/default-ffb67a2f790de55e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d85bbf1df59493f9f84b8b597325dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f544414e47a42d59367631af73dcb07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/brentdevries/.cache/huggingface/datasets/json/default-ffb67a2f790de55e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbfd81be30743a69c781bf0a7141cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets using BERT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63898abf8d7b4c47aac49ef9ae3ae5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bf60c35cc945bb95badf24d2c6b066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT sequence classification model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running training *****\n",
      "  Num examples = 1116\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sent to cuda\n",
      "Setting up training configuration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='1400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/1400 14:33, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Micro Precision</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Micro Recall</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.060147</td>\n",
       "      <td>0.542714</td>\n",
       "      <td>0.542714</td>\n",
       "      <td>0.315408</td>\n",
       "      <td>0.542714</td>\n",
       "      <td>0.390092</td>\n",
       "      <td>0.542714</td>\n",
       "      <td>0.297980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.963966</td>\n",
       "      <td>0.753769</td>\n",
       "      <td>0.753769</td>\n",
       "      <td>0.332971</td>\n",
       "      <td>0.753769</td>\n",
       "      <td>0.378769</td>\n",
       "      <td>0.753769</td>\n",
       "      <td>0.354057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.917678</td>\n",
       "      <td>0.773869</td>\n",
       "      <td>0.773869</td>\n",
       "      <td>0.340741</td>\n",
       "      <td>0.773869</td>\n",
       "      <td>0.367162</td>\n",
       "      <td>0.773869</td>\n",
       "      <td>0.347582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.864639</td>\n",
       "      <td>0.773869</td>\n",
       "      <td>0.773869</td>\n",
       "      <td>0.270175</td>\n",
       "      <td>0.773869</td>\n",
       "      <td>0.326964</td>\n",
       "      <td>0.773869</td>\n",
       "      <td>0.295869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.801549</td>\n",
       "      <td>0.809045</td>\n",
       "      <td>0.809045</td>\n",
       "      <td>0.575991</td>\n",
       "      <td>0.809045</td>\n",
       "      <td>0.492742</td>\n",
       "      <td>0.809045</td>\n",
       "      <td>0.464721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.730941</td>\n",
       "      <td>0.829146</td>\n",
       "      <td>0.829146</td>\n",
       "      <td>0.689242</td>\n",
       "      <td>0.829146</td>\n",
       "      <td>0.542125</td>\n",
       "      <td>0.829146</td>\n",
       "      <td>0.543434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.689271</td>\n",
       "      <td>0.844221</td>\n",
       "      <td>0.844221</td>\n",
       "      <td>0.685294</td>\n",
       "      <td>0.844221</td>\n",
       "      <td>0.589384</td>\n",
       "      <td>0.844221</td>\n",
       "      <td>0.609567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.874900</td>\n",
       "      <td>0.634840</td>\n",
       "      <td>0.844221</td>\n",
       "      <td>0.844221</td>\n",
       "      <td>0.623793</td>\n",
       "      <td>0.844221</td>\n",
       "      <td>0.599953</td>\n",
       "      <td>0.844221</td>\n",
       "      <td>0.610078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.874900</td>\n",
       "      <td>0.666780</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.705943</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.588346</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.628016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.874900</td>\n",
       "      <td>0.623642</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.662882</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.636644</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.646663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.874900</td>\n",
       "      <td>0.774245</td>\n",
       "      <td>0.854271</td>\n",
       "      <td>0.854271</td>\n",
       "      <td>0.653999</td>\n",
       "      <td>0.854271</td>\n",
       "      <td>0.584100</td>\n",
       "      <td>0.854271</td>\n",
       "      <td>0.610672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.874900</td>\n",
       "      <td>0.772156</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.709127</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.628544</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.655448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.874900</td>\n",
       "      <td>0.871397</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.652890</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.679722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.874900</td>\n",
       "      <td>1.001086</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.709127</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.628544</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.655448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>1.115410</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.720414</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.630668</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.661894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>1.177587</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.722056</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.650767</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.672904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>1.140214</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.727124</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.643013</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.673787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>1.142744</td>\n",
       "      <td>0.879397</td>\n",
       "      <td>0.879397</td>\n",
       "      <td>0.740065</td>\n",
       "      <td>0.879397</td>\n",
       "      <td>0.665236</td>\n",
       "      <td>0.879397</td>\n",
       "      <td>0.691620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>1.359900</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.725970</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.640544</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.667309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>1.061522</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.698541</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.669089</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.681734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-certainty/checkpoint-500\n",
      "Configuration saved in cmed-certainty/checkpoint-500/config.json\n",
      "Model weights saved in cmed-certainty/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-certainty/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in cmed-certainty/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-certainty/checkpoint-1000\n",
      "Configuration saved in cmed-certainty/checkpoint-1000/config.json\n",
      "Model weights saved in cmed-certainty/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-certainty/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in cmed-certainty/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_model(\"certainty\", model_checkpoints[model_type], epochs=20, lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5d900e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ffb67a2f790de55e\n",
      "Reusing dataset json (/home/brentdevries/.cache/huggingface/datasets/json/default-ffb67a2f790de55e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning data preprocessing\n",
      "Processed data written to certainty_sentence_train_for_sequence_classification.json and certainty_sentence_dev_for_sequence_classification.json\n",
      "Train label distribution:\n",
      "Certain: 932\n",
      "Hypothetical: 103\n",
      "Conditional: 81\n",
      "Dev label distribution:\n",
      "Certain: 157\n",
      "Hypothetical: 27\n",
      "Conditional: 15\n",
      "Loading dataset into huggingface format\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d8680bebfb45a7b2dff0a093f22ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets using BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/brentdevries/.cache/huggingface/datasets/json/default-ffb67a2f790de55e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-3aa847d265d25432.arrow\n",
      "Loading cached processed dataset at /home/brentdevries/.cache/huggingface/datasets/json/default-ffb67a2f790de55e/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-087973018a5a8c32.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT sequence classification model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running training *****\n",
      "  Num examples = 1116\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sent to cuda\n",
      "Setting up training configuration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1400' max='1400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/1400 15:32, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Micro Precision</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Micro Recall</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.121349</td>\n",
       "      <td>0.135678</td>\n",
       "      <td>0.135678</td>\n",
       "      <td>0.045226</td>\n",
       "      <td>0.135678</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.135678</td>\n",
       "      <td>0.079646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.074450</td>\n",
       "      <td>0.140704</td>\n",
       "      <td>0.140704</td>\n",
       "      <td>0.378788</td>\n",
       "      <td>0.140704</td>\n",
       "      <td>0.335456</td>\n",
       "      <td>0.140704</td>\n",
       "      <td>0.084219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.015983</td>\n",
       "      <td>0.603015</td>\n",
       "      <td>0.603015</td>\n",
       "      <td>0.374357</td>\n",
       "      <td>0.603015</td>\n",
       "      <td>0.479673</td>\n",
       "      <td>0.603015</td>\n",
       "      <td>0.371769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.949581</td>\n",
       "      <td>0.788945</td>\n",
       "      <td>0.788945</td>\n",
       "      <td>0.262982</td>\n",
       "      <td>0.788945</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.788945</td>\n",
       "      <td>0.294007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.886900</td>\n",
       "      <td>0.788945</td>\n",
       "      <td>0.788945</td>\n",
       "      <td>0.262982</td>\n",
       "      <td>0.788945</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.788945</td>\n",
       "      <td>0.294007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.820793</td>\n",
       "      <td>0.814070</td>\n",
       "      <td>0.814070</td>\n",
       "      <td>0.605585</td>\n",
       "      <td>0.814070</td>\n",
       "      <td>0.405284</td>\n",
       "      <td>0.814070</td>\n",
       "      <td>0.420063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.761554</td>\n",
       "      <td>0.829146</td>\n",
       "      <td>0.829146</td>\n",
       "      <td>0.606686</td>\n",
       "      <td>0.829146</td>\n",
       "      <td>0.472643</td>\n",
       "      <td>0.829146</td>\n",
       "      <td>0.503748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.986600</td>\n",
       "      <td>0.708167</td>\n",
       "      <td>0.829146</td>\n",
       "      <td>0.829146</td>\n",
       "      <td>0.596768</td>\n",
       "      <td>0.829146</td>\n",
       "      <td>0.553731</td>\n",
       "      <td>0.829146</td>\n",
       "      <td>0.569239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.986600</td>\n",
       "      <td>0.708125</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.732843</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.566470</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.615668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.986600</td>\n",
       "      <td>0.645566</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.731545</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.661681</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.689475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.986600</td>\n",
       "      <td>0.724869</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.698551</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.589038</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.620498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.986600</td>\n",
       "      <td>0.719013</td>\n",
       "      <td>0.854271</td>\n",
       "      <td>0.854271</td>\n",
       "      <td>0.614405</td>\n",
       "      <td>0.854271</td>\n",
       "      <td>0.554470</td>\n",
       "      <td>0.854271</td>\n",
       "      <td>0.573489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.986600</td>\n",
       "      <td>0.702375</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.621137</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.655513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.986600</td>\n",
       "      <td>0.676069</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.676148</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.616545</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.640062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.488000</td>\n",
       "      <td>0.765123</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.681723</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.606322</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.633595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.488000</td>\n",
       "      <td>0.817896</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.722056</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.650767</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.672904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.488000</td>\n",
       "      <td>0.945863</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.692888</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.608445</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.639604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.488000</td>\n",
       "      <td>0.939732</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.664859</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.586223</td>\n",
       "      <td>0.859296</td>\n",
       "      <td>0.616163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.488000</td>\n",
       "      <td>0.988001</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.652890</td>\n",
       "      <td>0.874372</td>\n",
       "      <td>0.679722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.488000</td>\n",
       "      <td>1.025251</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.722056</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.650767</td>\n",
       "      <td>0.869347</td>\n",
       "      <td>0.672904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-certainty/checkpoint-500\n",
      "Configuration saved in cmed-certainty/checkpoint-500/config.json\n",
      "Model weights saved in cmed-certainty/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-certainty/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in cmed-certainty/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-certainty/checkpoint-1000\n",
      "Configuration saved in cmed-certainty/checkpoint-1000/config.json\n",
      "Model weights saved in cmed-certainty/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-certainty/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in cmed-certainty/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 199\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_model(\"certainty\", model_checkpoints[model_type], epochs=20, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f08ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
