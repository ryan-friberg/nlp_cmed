{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7243dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_model\n",
    "from run_model import run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee2f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"domain\" # choose checkpoint type\n",
    "model_checkpoints = {\n",
    "    \"fast\": \"distilbert-base-uncased\",\n",
    "    \"base\": \"bert-base-uncased\",\n",
    "    \"domain\": \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f86c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning data preprocessing\n",
      "Processed data written to temporality_tokens200_train_for_token_classification.json and temporality_tokens200_dev_for_token_classification.json\n",
      "Train label distribution:\n",
      "Past: 570\n",
      "Present: 417\n",
      "Future: 107\n",
      "Dev label distribution:\n",
      "Past: 119\n",
      "Present: 48\n",
      "Future: 29\n",
      "Loading dataset into huggingface format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8b14faa69734d3cd\n",
      "Reusing dataset json (/home/brentdevries/.cache/huggingface/datasets/json/default-8b14faa69734d3cd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72668f413b54c96916002f96cbbdb5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets using BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/brentdevries/.cache/huggingface/datasets/json/default-8b14faa69734d3cd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-1e33113a53ade1dd.arrow\n",
      "Loading cached processed dataset at /home/brentdevries/.cache/huggingface/datasets/json/default-8b14faa69734d3cd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-4c79b3a813367783.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT token classification model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running training *****\n",
      "  Num examples = 531\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sent to cuda\n",
      "Setting up training configuration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3300' max='3300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3300/3300 1:02:52, Epoch 99/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Micro Precision</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Micro Recall</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.162562</td>\n",
       "      <td>0.241026</td>\n",
       "      <td>0.241026</td>\n",
       "      <td>0.305842</td>\n",
       "      <td>0.241026</td>\n",
       "      <td>0.290603</td>\n",
       "      <td>0.241026</td>\n",
       "      <td>0.202451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.160214</td>\n",
       "      <td>0.251282</td>\n",
       "      <td>0.251282</td>\n",
       "      <td>0.311349</td>\n",
       "      <td>0.251282</td>\n",
       "      <td>0.296253</td>\n",
       "      <td>0.251282</td>\n",
       "      <td>0.212373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.157102</td>\n",
       "      <td>0.251282</td>\n",
       "      <td>0.251282</td>\n",
       "      <td>0.311973</td>\n",
       "      <td>0.251282</td>\n",
       "      <td>0.296253</td>\n",
       "      <td>0.251282</td>\n",
       "      <td>0.212369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.152742</td>\n",
       "      <td>0.261538</td>\n",
       "      <td>0.261538</td>\n",
       "      <td>0.308029</td>\n",
       "      <td>0.261538</td>\n",
       "      <td>0.301903</td>\n",
       "      <td>0.261538</td>\n",
       "      <td>0.222632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.147084</td>\n",
       "      <td>0.287179</td>\n",
       "      <td>0.287179</td>\n",
       "      <td>0.331575</td>\n",
       "      <td>0.287179</td>\n",
       "      <td>0.320147</td>\n",
       "      <td>0.287179</td>\n",
       "      <td>0.245150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.141379</td>\n",
       "      <td>0.287179</td>\n",
       "      <td>0.287179</td>\n",
       "      <td>0.311560</td>\n",
       "      <td>0.287179</td>\n",
       "      <td>0.303238</td>\n",
       "      <td>0.287179</td>\n",
       "      <td>0.237530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.135986</td>\n",
       "      <td>0.297436</td>\n",
       "      <td>0.297436</td>\n",
       "      <td>0.327831</td>\n",
       "      <td>0.297436</td>\n",
       "      <td>0.313007</td>\n",
       "      <td>0.297436</td>\n",
       "      <td>0.245848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.129590</td>\n",
       "      <td>0.328205</td>\n",
       "      <td>0.328205</td>\n",
       "      <td>0.386961</td>\n",
       "      <td>0.328205</td>\n",
       "      <td>0.334076</td>\n",
       "      <td>0.328205</td>\n",
       "      <td>0.270885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.123023</td>\n",
       "      <td>0.348718</td>\n",
       "      <td>0.348718</td>\n",
       "      <td>0.414815</td>\n",
       "      <td>0.348718</td>\n",
       "      <td>0.345376</td>\n",
       "      <td>0.348718</td>\n",
       "      <td>0.286602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.116895</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.452227</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.339531</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.274404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.109375</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.449658</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.335411</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.274315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.105311</td>\n",
       "      <td>0.379487</td>\n",
       "      <td>0.379487</td>\n",
       "      <td>0.292795</td>\n",
       "      <td>0.379487</td>\n",
       "      <td>0.336747</td>\n",
       "      <td>0.379487</td>\n",
       "      <td>0.272896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.100310</td>\n",
       "      <td>0.405128</td>\n",
       "      <td>0.405128</td>\n",
       "      <td>0.304578</td>\n",
       "      <td>0.405128</td>\n",
       "      <td>0.350871</td>\n",
       "      <td>0.405128</td>\n",
       "      <td>0.289827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.095849</td>\n",
       "      <td>0.441026</td>\n",
       "      <td>0.441026</td>\n",
       "      <td>0.319810</td>\n",
       "      <td>0.441026</td>\n",
       "      <td>0.370645</td>\n",
       "      <td>0.441026</td>\n",
       "      <td>0.312757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.090253</td>\n",
       "      <td>0.441026</td>\n",
       "      <td>0.441026</td>\n",
       "      <td>0.314069</td>\n",
       "      <td>0.441026</td>\n",
       "      <td>0.362406</td>\n",
       "      <td>0.441026</td>\n",
       "      <td>0.310482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.073000</td>\n",
       "      <td>1.087734</td>\n",
       "      <td>0.456410</td>\n",
       "      <td>0.456410</td>\n",
       "      <td>0.323116</td>\n",
       "      <td>0.456410</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.456410</td>\n",
       "      <td>0.321149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.073000</td>\n",
       "      <td>1.081100</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.316496</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.365466</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.073000</td>\n",
       "      <td>1.078524</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.314042</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.365466</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.319052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.073000</td>\n",
       "      <td>1.073348</td>\n",
       "      <td>0.497436</td>\n",
       "      <td>0.497436</td>\n",
       "      <td>0.328070</td>\n",
       "      <td>0.497436</td>\n",
       "      <td>0.385240</td>\n",
       "      <td>0.497436</td>\n",
       "      <td>0.339941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.073000</td>\n",
       "      <td>1.070218</td>\n",
       "      <td>0.502564</td>\n",
       "      <td>0.502564</td>\n",
       "      <td>0.332912</td>\n",
       "      <td>0.502564</td>\n",
       "      <td>0.392185</td>\n",
       "      <td>0.502564</td>\n",
       "      <td>0.344683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.073000</td>\n",
       "      <td>1.062668</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>0.338095</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>0.399364</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>0.354650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.073000</td>\n",
       "      <td>1.058596</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.349041</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.413253</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.364389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.073000</td>\n",
       "      <td>1.049656</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.352167</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.417608</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.371329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.073000</td>\n",
       "      <td>1.043744</td>\n",
       "      <td>0.558974</td>\n",
       "      <td>0.558974</td>\n",
       "      <td>0.356234</td>\n",
       "      <td>0.558974</td>\n",
       "      <td>0.423258</td>\n",
       "      <td>0.558974</td>\n",
       "      <td>0.377189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.073000</td>\n",
       "      <td>1.040725</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.360508</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.425847</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.375244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.073000</td>\n",
       "      <td>1.028856</td>\n",
       "      <td>0.574359</td>\n",
       "      <td>0.574359</td>\n",
       "      <td>0.382145</td>\n",
       "      <td>0.574359</td>\n",
       "      <td>0.456450</td>\n",
       "      <td>0.574359</td>\n",
       "      <td>0.397063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.073000</td>\n",
       "      <td>1.016769</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>0.377210</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>0.449506</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>0.392409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.073000</td>\n",
       "      <td>0.998183</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.394741</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.474694</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.413939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.073000</td>\n",
       "      <td>0.972637</td>\n",
       "      <td>0.635897</td>\n",
       "      <td>0.635897</td>\n",
       "      <td>0.415998</td>\n",
       "      <td>0.635897</td>\n",
       "      <td>0.506827</td>\n",
       "      <td>0.635897</td>\n",
       "      <td>0.440379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.073000</td>\n",
       "      <td>0.950884</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.419235</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.509652</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.443396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.934700</td>\n",
       "      <td>0.921683</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>0.431552</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>0.524835</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>0.450781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.934700</td>\n",
       "      <td>0.893510</td>\n",
       "      <td>0.682051</td>\n",
       "      <td>0.682051</td>\n",
       "      <td>0.458772</td>\n",
       "      <td>0.682051</td>\n",
       "      <td>0.561088</td>\n",
       "      <td>0.682051</td>\n",
       "      <td>0.478133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.934700</td>\n",
       "      <td>0.850269</td>\n",
       "      <td>0.687179</td>\n",
       "      <td>0.687179</td>\n",
       "      <td>0.457325</td>\n",
       "      <td>0.687179</td>\n",
       "      <td>0.559793</td>\n",
       "      <td>0.687179</td>\n",
       "      <td>0.480077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.934700</td>\n",
       "      <td>0.861120</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>0.468387</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>0.566502</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>0.477231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.934700</td>\n",
       "      <td>0.819416</td>\n",
       "      <td>0.697436</td>\n",
       "      <td>0.697436</td>\n",
       "      <td>0.469123</td>\n",
       "      <td>0.697436</td>\n",
       "      <td>0.573682</td>\n",
       "      <td>0.697436</td>\n",
       "      <td>0.488897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.934700</td>\n",
       "      <td>0.804803</td>\n",
       "      <td>0.687179</td>\n",
       "      <td>0.687179</td>\n",
       "      <td>0.469780</td>\n",
       "      <td>0.687179</td>\n",
       "      <td>0.568032</td>\n",
       "      <td>0.687179</td>\n",
       "      <td>0.482456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.934700</td>\n",
       "      <td>0.774983</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.476140</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.579331</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.495157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.934700</td>\n",
       "      <td>0.790012</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.471331</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.570857</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.485630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.934700</td>\n",
       "      <td>0.765609</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.476140</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.579331</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.495157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.934700</td>\n",
       "      <td>0.756410</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.476140</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.579331</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.495157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.934700</td>\n",
       "      <td>0.752666</td>\n",
       "      <td>0.702564</td>\n",
       "      <td>0.702564</td>\n",
       "      <td>0.474510</td>\n",
       "      <td>0.702564</td>\n",
       "      <td>0.576507</td>\n",
       "      <td>0.702564</td>\n",
       "      <td>0.491980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.934700</td>\n",
       "      <td>0.740972</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.476140</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.579331</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.495157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.934700</td>\n",
       "      <td>0.742197</td>\n",
       "      <td>0.702564</td>\n",
       "      <td>0.702564</td>\n",
       "      <td>0.474510</td>\n",
       "      <td>0.702564</td>\n",
       "      <td>0.576507</td>\n",
       "      <td>0.702564</td>\n",
       "      <td>0.491980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.934700</td>\n",
       "      <td>0.723679</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.479487</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.584981</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.501519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.934700</td>\n",
       "      <td>0.724360</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.484737</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.593456</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.511088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.715290</td>\n",
       "      <td>0.728205</td>\n",
       "      <td>0.728205</td>\n",
       "      <td>0.479706</td>\n",
       "      <td>0.728205</td>\n",
       "      <td>0.586511</td>\n",
       "      <td>0.728205</td>\n",
       "      <td>0.506549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.724169</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>0.481205</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>0.587806</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>0.504704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.710828</td>\n",
       "      <td>0.728205</td>\n",
       "      <td>0.728205</td>\n",
       "      <td>0.479706</td>\n",
       "      <td>0.728205</td>\n",
       "      <td>0.586511</td>\n",
       "      <td>0.728205</td>\n",
       "      <td>0.506549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.696348</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.606480</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.539914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.695234</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.816638</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.600830</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.533478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.701007</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.816638</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.600830</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.533478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.698210</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.821777</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.608010</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.544963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.691265</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.606480</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.539914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.709722</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.819787</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.604950</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.534821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.698646</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.825456</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.613425</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.544505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.696814</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.606480</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.539914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.709573</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.819120</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.603891</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.543488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.711901</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.824543</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.612130</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.546382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.706635</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.826118</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.613660</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.551462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.619300</td>\n",
       "      <td>0.708510</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.822385</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.617974</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.562289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.698394</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.824423</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.620799</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.565542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.723135</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.822508</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.609305</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.543144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.717794</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.830792</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.629274</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.575359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.701453</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.740843</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.626644</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.579837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.693307</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.714897</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.635313</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.595382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.728790</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.823432</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.619269</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.560384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.725987</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.691987</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.622760</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.585394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.749090</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.814129</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.605616</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.557921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.728168</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.696763</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.630999</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.588299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.734045</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.701595</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.637943</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.593016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.739279</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.687850</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.619699</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.575026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.725160</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.689954</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.638373</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.605229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.744120</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.685383</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.630134</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.602213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.742347</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.675439</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.617540</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.590577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.466700</td>\n",
       "      <td>0.747997</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.680291</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.625779</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.593709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.774025</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.680291</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.625779</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.593709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.787433</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.677811</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.621660</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.592175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.797667</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.658582</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.625544</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.585992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.760666</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.587087</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.586937</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.564895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.803379</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.605855</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.591057</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.567885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.805830</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.608349</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.601062</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.582854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.849652</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.575762</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.586312</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.550142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.838243</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.608349</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.601062</td>\n",
       "      <td>0.758974</td>\n",
       "      <td>0.582854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.857903</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.591849</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.590862</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.564167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.856284</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.582716</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.582152</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.547157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.906733</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.575732</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.586312</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.551733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.927248</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.577518</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.590432</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.552849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.955339</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.566820</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.580898</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.552528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.945892</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.553922</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.569599</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.538291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>1.004609</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.580331</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.593257</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.556234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>1.045783</td>\n",
       "      <td>0.702564</td>\n",
       "      <td>0.702564</td>\n",
       "      <td>0.541380</td>\n",
       "      <td>0.702564</td>\n",
       "      <td>0.565008</td>\n",
       "      <td>0.702564</td>\n",
       "      <td>0.523154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>1.051799</td>\n",
       "      <td>0.712821</td>\n",
       "      <td>0.712821</td>\n",
       "      <td>0.536724</td>\n",
       "      <td>0.712821</td>\n",
       "      <td>0.566538</td>\n",
       "      <td>0.712821</td>\n",
       "      <td>0.528890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>1.076614</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.547132</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.569599</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.538088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>1.108376</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>0.536850</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>0.563949</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>0.531846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>1.049217</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.530844</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.561124</td>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.528986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>1.226688</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.575626</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.586312</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.551161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>1.160993</td>\n",
       "      <td>0.712821</td>\n",
       "      <td>0.712821</td>\n",
       "      <td>0.529232</td>\n",
       "      <td>0.712821</td>\n",
       "      <td>0.554180</td>\n",
       "      <td>0.712821</td>\n",
       "      <td>0.523370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>1.238724</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.571055</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.579368</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.546599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>1.209003</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.550097</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.572424</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.541955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>1.198160</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>0.539428</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>0.559829</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>0.530022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-temporality/checkpoint-500\n",
      "Configuration saved in cmed-temporality/checkpoint-500/config.json\n",
      "Model weights saved in cmed-temporality/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-temporality/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in cmed-temporality/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to cmed-temporality/checkpoint-1000\n",
      "Configuration saved in cmed-temporality/checkpoint-1000/config.json\n",
      "Model weights saved in cmed-temporality/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-temporality/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in cmed-temporality/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to cmed-temporality/checkpoint-1500\n",
      "Configuration saved in cmed-temporality/checkpoint-1500/config.json\n",
      "Model weights saved in cmed-temporality/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-temporality/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in cmed-temporality/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-temporality/checkpoint-2000\n",
      "Configuration saved in cmed-temporality/checkpoint-2000/config.json\n",
      "Model weights saved in cmed-temporality/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in cmed-temporality/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in cmed-temporality/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-temporality/checkpoint-2500\n",
      "Configuration saved in cmed-temporality/checkpoint-2500/config.json\n",
      "Model weights saved in cmed-temporality/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-temporality/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in cmed-temporality/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-temporality/checkpoint-3000\n",
      "Configuration saved in cmed-temporality/checkpoint-3000/config.json\n",
      "Model weights saved in cmed-temporality/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-temporality/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in cmed-temporality/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: temporality_tags, tokens.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 87\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_model(\"temporality\", model_checkpoints[model_type], epochs=100, classification=\"token\", chunk_by=\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b06e777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning data preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-91633e76452b96ad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data written to temporality_sentence_train_for_sequence_classification.json and temporality_sentence_dev_for_sequence_classification.json\n",
      "Train label distribution:\n",
      "Past: 570\n",
      "Present: 417\n",
      "Future: 107\n",
      "Dev label distribution:\n",
      "Past: 119\n",
      "Present: 48\n",
      "Future: 29\n",
      "Loading dataset into huggingface format\n",
      "Downloading and preparing dataset json/default to /home/brentdevries/.cache/huggingface/datasets/json/default-91633e76452b96ad/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4cf9850baa4f76ab79c19f9b96bbd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf2d33e711648b9b1444ce1fa8f6e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/brentdevries/.cache/huggingface/datasets/json/default-91633e76452b96ad/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fe11627d5e4b29aab9d15b92cd7dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets using BERT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8824d4022b4846dc9cfe044ad904c9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b786e6c92289446b97080705e3a347d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT sequence classification model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running training *****\n",
      "  Num examples = 1094\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sent to cuda\n",
      "Setting up training configuration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1360' max='1360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1360/1360 13:26, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Micro Precision</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Micro Recall</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.361935</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.116780</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.365900</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.157828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.173188</td>\n",
       "      <td>0.255102</td>\n",
       "      <td>0.255102</td>\n",
       "      <td>0.374948</td>\n",
       "      <td>0.255102</td>\n",
       "      <td>0.340155</td>\n",
       "      <td>0.255102</td>\n",
       "      <td>0.202567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.973726</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.359988</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.424370</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.386854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.859851</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.400840</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.481384</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.431250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.745824</td>\n",
       "      <td>0.698980</td>\n",
       "      <td>0.698980</td>\n",
       "      <td>0.443945</td>\n",
       "      <td>0.698980</td>\n",
       "      <td>0.541200</td>\n",
       "      <td>0.698980</td>\n",
       "      <td>0.478134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.689752</td>\n",
       "      <td>0.704082</td>\n",
       "      <td>0.704082</td>\n",
       "      <td>0.443103</td>\n",
       "      <td>0.704082</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.704082</td>\n",
       "      <td>0.476862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.653695</td>\n",
       "      <td>0.709184</td>\n",
       "      <td>0.709184</td>\n",
       "      <td>0.451389</td>\n",
       "      <td>0.709184</td>\n",
       "      <td>0.546802</td>\n",
       "      <td>0.709184</td>\n",
       "      <td>0.483405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.887600</td>\n",
       "      <td>0.634676</td>\n",
       "      <td>0.724490</td>\n",
       "      <td>0.724490</td>\n",
       "      <td>0.788302</td>\n",
       "      <td>0.724490</td>\n",
       "      <td>0.559755</td>\n",
       "      <td>0.724490</td>\n",
       "      <td>0.511483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.887600</td>\n",
       "      <td>0.624156</td>\n",
       "      <td>0.760204</td>\n",
       "      <td>0.760204</td>\n",
       "      <td>0.813786</td>\n",
       "      <td>0.760204</td>\n",
       "      <td>0.648095</td>\n",
       "      <td>0.760204</td>\n",
       "      <td>0.638139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.887600</td>\n",
       "      <td>0.625815</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.746381</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.670961</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.684798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.887600</td>\n",
       "      <td>0.607655</td>\n",
       "      <td>0.790816</td>\n",
       "      <td>0.790816</td>\n",
       "      <td>0.725036</td>\n",
       "      <td>0.790816</td>\n",
       "      <td>0.717874</td>\n",
       "      <td>0.790816</td>\n",
       "      <td>0.713093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.887600</td>\n",
       "      <td>0.595320</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.721016</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.743424</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.730495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.887600</td>\n",
       "      <td>0.616196</td>\n",
       "      <td>0.780612</td>\n",
       "      <td>0.780612</td>\n",
       "      <td>0.714423</td>\n",
       "      <td>0.780612</td>\n",
       "      <td>0.700248</td>\n",
       "      <td>0.780612</td>\n",
       "      <td>0.704258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.887600</td>\n",
       "      <td>0.639118</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.714078</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.708651</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.710245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>0.739241</td>\n",
       "      <td>0.780612</td>\n",
       "      <td>0.780612</td>\n",
       "      <td>0.699494</td>\n",
       "      <td>0.780612</td>\n",
       "      <td>0.706017</td>\n",
       "      <td>0.780612</td>\n",
       "      <td>0.699789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>0.831859</td>\n",
       "      <td>0.780612</td>\n",
       "      <td>0.780612</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>0.780612</td>\n",
       "      <td>0.708535</td>\n",
       "      <td>0.780612</td>\n",
       "      <td>0.700825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>0.875602</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.722768</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.729774</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.723215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>0.920054</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.707283</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.707192</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.704517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>0.994319</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.715545</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.734731</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.724014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.324700</td>\n",
       "      <td>1.059967</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.704779</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.719622</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.707539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to cmed-temporality/checkpoint-500\n",
      "Configuration saved in cmed-temporality/checkpoint-500/config.json\n",
      "Model weights saved in cmed-temporality/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-temporality/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in cmed-temporality/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-temporality/checkpoint-1000\n",
      "Configuration saved in cmed-temporality/checkpoint-1000/config.json\n",
      "Model weights saved in cmed-temporality/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-temporality/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in cmed-temporality/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAEGCAYAAAAg6I3HAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAh+0lEQVR4nO3de5xVdb3/8dd7hgFEBAUUES9geQmVkBDQSqc0s9LwdEotTTJLMcNTeck8djT72fGU6fGk/RLv5hXN0n79EhLlhIYSKoVoKikHEeIyXARELns+54+1RrfIzGxm9pq91/B+Ph7rsff+7rW/67O2w8fv/q7v97sUEZiZWfnVVDoAM7POygnWzCwjTrBmZhlxgjUzy4gTrJlZRrpUOoBq0a9PbQzao67SYVStl1/aqdIhVL+NGysdQVVb17iGDY1vqT11fPJj20fD8kJJ+z791/WTIuKY9hyvvZxgU4P2qGPGpD0qHUbV+vQnTqx0CNXvtUWVjqCqTV/zYLvrWLa8wFOTdi9p37oBf+/X0vuSbgaOBZZExIFpWR/gXmAQMA84ISJWpO99DzgdKADnRMSk1mJwF4GZ5UhQiMaSthLcCmzewr0QmBIR+wBT0tdIGgKcBByQfubnkmpbO4ATrJnlRgCNRElbq3VF/BFYvlnxGOC29PltwPFF5fdExPqIeBWYC4xs7RjuIjCzXGmkpNZpW/WPiEUAEbFI0i5p+UDgyaL9FqRlLXKCNbPcCIKNpf38B+gnaWbR6wkRMaGNh97SxblWm8lOsGaWGwEUSvj5n1oWESO28hCLJQ1IW68DgCVp+QKg+Cr47sDC1ipzH6yZ5Uq5+mCb8RAwNn0+FniwqPwkSd0kDQb2AWa0VplbsGaWGwEUyrQCoKS7gXqSroQFwCXAFcBESacD84EvAETEHEkTgeeBTcDZEdHqgFwnWDPLlXJd4oqILzbz1pHN7H85cPnWHMMJ1sxyI4it6YOtOCdYM8uNCNiYn/zqBGtmeSIKWxwxVZ2cYM0sNwJodAvWzCwbbsGamWUgmWjgBGtmVnYBbIz8zI9ygjWz3AhEIUcTUJ1gzSxXGsNdBGZmZec+WDOzzIiC+2DNzMovuaOBE6yZWdlFiA3R6q2wqoYTrJnlSqP7YM3Myi+5yOUuAjOzDPgil5lZJnyRy8wsQwVPNDAzK79AbIz8pK38RGpm2zxf5DIzy0ggdxGYmWXFF7msVT/99h489Ugvduy3iQmPvQjAGytq+dG4QSxe0JX+u2/gX6+fxw47Fnj6v3ty8492Y9NG0aUu+Pr3FzLsI2sqfAYd51vnzmDkqEWsXNmNb5xxDABfHjub0YctpDHEqpXduOonI1nesF2FI60eNTXBNfc/S8OSblw67oBKh1M2EeRqmFZVRyqpIGmWpOck3Sepx1Z+fpCkL2UVX3scfeJyLr/zlXeVTbx2Fw7+yGpueeIFDv7Iau69dhcAevcpcNltr3D9oy9y/jXz+fE5e1Yi5Ip5ZPJgvn/R4e8qu/++/Tn7zE8yftzRzHhyAF86ZU6FoqtOY059ndde2ap/LrmQXOSqLWmrBlWdYIF1ETEsIg4ENgDjtvLzg4CqTLAHjV7LDjsV3lU2fVJvjjphOQBHnbCc6Q/3BuD9B62j766bANhrv7fYsL6GDevz0w/VXs/N3pnVq7u+q2zdm3VvP+/evUDkqF8ua337r+eQI5Yz6b5dKx1KJgrUlLRVgzx1EUwDhko6DrgY6Ao0ACdHxGJJRwDXpPsGcDhwBfABSbOA2yLi6o4Pu3QrltXRt3+SSPv238TKhvf+53n8d7153wHr6NotR7fWzMipp83myKPmsXZtHReeX1/pcKrGmRf9nZuvHMx22xda3zlnAuVqwe3qSPOtkNQF+BQwG3gcGB0RBwP3ABeku50HnB0Rw4CPAuuAC4FpaSu4qpNrKea92J2bLt+Nf/nxa5UOpSrcfstBjD35OKY+uhfHjZlb6XCqwsj6BlY2dGXunB0qHUpm8tSCrY4omrdd2vqcCcwHbgJ2ByZJmg2cDzT14D8BXCXpHGDHiNjUWuWSzpA0U9LMpQ2V/7/9Tv020rA4abU2LO7Cjn3fOYWlC+u47PRBnH/NfHYbtKFSIValqY/uyYc/sqDSYVSFIcPfYPTHG7hlygy++9O/MXTUSs778d8qHVbZBNAYNSVt1aA6omheUx/ssIgYHxEbgJ8B10bEQcCZQHeAiLgC+BqwHfCkpP1bqzwiJkTEiIgYsXPfyneKjz76DR6Z2AeARyb24dBPrgJgzapavn/q3pz2vUUcMHJtJUOsGrsNXP3281GHLmTBa70qGE31uPWqwZxaP4rTjhzJf5y7P399akeuvKDVfwo5IgolbtUgT32wTXoDr6fPxzYVSnpfRMwGZks6FNgfeA2oyt9K/37WXvx1ek9WLe/CyR8awpfP/QcnfnMxl48bxMP39GWXgckwLYCHbunHwle7ctfVu3LX1cmFi3+/5+/s2K/VRnqncMFF0xk6dCm9eq/n9rt+yx23H8AhIxcxcPfVRIgli3tw7TUfqnSY1gGS23ZXvjFUKkVU78USSWsioudmZWOAq0mS7JPAIRFRL+lnwMeAAvA88BWgEXgY6Afc2lI/7IgPdo8Zk/bI5Dw6g09/4sRKh1D9XltU6Qiq2vQ1D7Jq07J2NS0HHrBjfGPiR0ra9+IDf/d0RIxoz/Haq6pbsJsn17TsQeDBLZSPb6aaI8sdl5lVTp4mGlR1gjUzK5asB1sd/aulcII1sxzJ1x0N8hOpmW3zkmFaKmlrjaRvS5qTTsW/W1J3SX0k/UHSy+njTu2J1wnWzHKjXGsRSBoInAOMSKfi1wInkUxOmhIR+wBT0tdt5gRrZrnSSE1JWwm6kExm6gL0ABYCY4Db0vdvA45vT6zugzWz3EiWKyz5Ilc/STOLXk+IiAlJPfG6pCtJZoiuAyZHxGRJ/SNiUbrPIkm7tCdeJ1gzy5WtWOxlWXPjYNO+1THAYGAlcJ+kU8oSYBEnWDPLjWQ1rbL0bB4FvBoRSwEkPQAcBiyWNCBtvQ4AlrTnIE6wZpYbyVTZsiTY+cDodBH/dSQTkmYCa0mm4F+RPr5nUtPWcII1sxwpTws2Ip6SdD/wDLAJeBaYAPQEJko6nSQJf6E9x3GCNbNcKddMroi4BLhks+L1lHF6vROsmeXGVo4iqDgnWDPLlWpZTLsUTrBmlht5uyeXE6yZ5UYAm9yCNTPLhrsIzMyyUOJKWdXCCdbMcsMLbpuZZcgtWDOzDDQtuJ0XTrBmlhuB2NToi1xmZplwH6yZWRbCXQRmZplwH6yZWYacYM3MMhCIgi9ymZllwxe5zMwyEL7IZWaWnXCCNTPLghd7MTPLjFuwOfTynJ58ev/DKx1G1Vr05b6VDqHq7fbgmkqHUN3erG13FRFQaHSCNTPLhEcRmJllIHAXgZlZRnyRy8wsMxGVjqB0TrBmlivuIjAzy0AyisBrEZiZZcJdBGZmGXEXgZlZBgI5wZqZZSVHPQROsGaWIwHhqbJmZtlwF4GZWUY6xSgCST+jhe6OiDgnk4jMzJpR7rUIJO0I3AgcmFb/VeBF4F5gEDAPOCEiVrSl/pZasDPbUqGZWWYCKG8XwTXAwxHxeUldgR7ARcCUiLhC0oXAhcB321J5swk2Im4rfi1p+4hY25aDmJmVS7m6CCT1Ag4HvpLUGxuADZLGAPXpbrcBU2ljgm11zpmkQyU9D7yQvv6gpJ+35WBmZu0jorG0DegnaWbRdsZmle0NLAVukfSspBslbQ/0j4hFAOnjLm2NtpSLXP8JfBJ4KD3gXyR56X8zq4zSW7DLImJEC+93AYYD4yPiKUnXkHQHlE1JqyZExGubFRXKGYSZWUkiuchVylaCBcCCiHgqfX0/ScJdLGkAQPq4pK3hlpJgX5N0GBCSuko6j7S7wMysw0WJW2vVRPyDJL/tlxYdCTxP8mt9bFo2FniwraGW0kUwjuRK20DgdWAScHZbD2hm1j5lHUUwHrgzHUHwCnAaScNzoqTTgfnAF9paeasJNiKWASe39QBmZmXVWL6qImIWsKV+2iPLUX8powj2lvRbSUslLZH0oKS9y3FwM7Ot0jQOtpStCpTSB3sXMBEYAOwG3AfcnWVQZmbNiShtqwalJFhFxC8jYlO63UG+Vgwzs86kTBe5OkJLaxH0SZ8+lk4Xu4ck7BOB33VAbGZm71UlP/9L0dJFrqdJEmrT2ZxZ9F4AP8wqKDOz5qhKWqelaGktgsEdGYiZWatC0NkW3JZ0IDAE6N5UFhG3ZxWUmVmzOkMLtomkS0hWlhkC/H/gU8DjgBOsmXW8HCXYUkYRfJ5k0O0/IuI04INAt0yjMjNrTmcYRVBkXUQ0StqUrp+4hGSZL8tITU1wzf3P0rCkG5eOO6DS4VRc19pN3HTqg3StLVBb08gjf9ubX/xxJAAnjZjNiSNmU2isYdrcvbjm0UMrHG3lHf/FVzl6zGtEwP/M3YGrfziUjRtqKx1WeZR/we1MlZJgZ6a3VbiBZGTBGmBGax+SVABmp8d4ARgbEW+2PdStJ6ke2BARf+rI47bXmFNf57VXetCjpxctA9hQqOWMOz7Luo11dKkpcPOpv+GJuXvSra5A/b6vcsINJ7KxUMtOPTr0z6sq9d35LY47cR5nnXg4G9bXcuGPnuGITyzikd/tXunQyiZPowha7SKIiG9ExMqI+AXwCZJEeVoJda+LiGERcSCwgWTRmLdJ6oj/pdYDh3XAccqmb//1HHLEcibdt2ulQ6kiYt3GOgC61DTSpbaRQHxh+Bxu+dNwNhaSP6UVb/aoZJBVo7Y26NqtQE1tI926F2hY1sl69DpDF4Gk4S29FxHPbMVxpgFD0xblJcAiYJikg4ArSBJhN+C6iLg+XYPxXqBXGuNZETFN0tHAD9J9/w6cFhFrJM0jubXDcUAdyeo3b5Ek9YKkU0gW1Z22FTFXxJkX/Z2brxzMdtu79VqsRo3cdfr97LHTKu6deSDPLezPXn1XcvCeCzm7/ik2bKrlqimH8fyiNi8+3yk0LO3OA3cM5taHHmPD+lqeeaofzz61c6XDKqs8tWBb6iL4aQvvBfDxUg4gqQvJyIOH06KRwIER8Wp6C4dVEXGIpG7AE5ImA58DJkXE5WlLt4ekfsDFwFERsVbSd4HvAJel9S6LiOGSvgGcFxFfk/QLYE1EXNlMbGcAZwB01/alnE6mRtY3sLKhK3Pn7MBBI1dWOpyq0hg1nHTjCfTstp6rPv8w79u5gVo10qv7Bk699XMcsNsSfvy5yRx73cmUeTm7XOm5w0ZGH7GErx5fz9rVdXzvimf52DGv89jDAysdWvl0hj7YiPhYO+veTtKs9Pk04CaSn+szIuLVtPxokpbt59PXvYF9gD8DN0uqA34TEbMkHUEyVOwJSQBdgelFx3sgfXyaJEG3KiImABMAenfpV/H/Lw4Z/gajP97AIUcsp65rIz16Fjjvx3/jygv2r3RoVWPN+m7MnL8bh+39GotX92TK3wYDYs7C/jSG2KnHW6x4c7tKh1kxw0YuY/HC7XhjZdIt8KfH+vOBoSs6T4Ktop//pShpokEbrYuIYcUFaWIsvjOtSH66T9r8w+l9vz4D/FLST4AVwB8i4ovNHG99+lgg2/PKzK1XDebWq5IJdAeNXMk/f/V1J1dgpx7r2FioYc36bnTrsolRgxZw6/SDWbexjpGDXufp+QPZs89K6moLrHize+sVdmJL/7Ed+x24km7dCqxfX8MHD2lg7gu9Kx1WeTnBlmwScJakRyNio6R9Se6a0A94PSJuSO/yOBy4HLhO0vsjYq6kHsDuEfFSC/WvJunHtRzr1/NNLjvuUWrUSI2CP7zwfqbNHUSXmgKXHvsY9339HjY21vJvD32cbbl7AODFOTvyxJRdueaXj1MoiFde7MXvf71HpcMqK5Vxwe2sVTrB3ggMAp5R0rxdChxPctHrfEkbSYaFnRoRSyV9Bbg77a+FpE+2pQT7W+D+9D7nubjI1WT2jB2ZPWPHSodRFV5e0pcv3vTeu3Zsaqzl4oeOqkBE1e3OG/blzhv2rXQY2elMLdg08Z0M7B0Rl0naE9g1IlocCxsRPbdQNhWYWvS6Ebgo3Yrdlm6bf/5R4JAtlA8qej6TJEGTtm6HthSnmeWHIl+jCEqZKvtz4FCgqe9zNXBdZhGZmbUkR7eMKaWLYFQ6/OlZgIhYkd6B0cys4+WoBVtKgt2YjkUNAEk7U9b7OpqZlS5PXQSlJNj/An4N7CLpcpLVtS7ONCozsy2JTjaKICLulPQ0yZKFAo6PiBcyj8zMbEs6Uws2HTXwJsmQp7fLImJ+loGZmW1RZ0qwJHeQbbr5YXdgMPAi4IVKzazDdao+2Ig4qPh1usrWmc3sbmZmqa2eyRURz0h6z2B/M7MO0ZlasJK+U/SyhmRdgKWZRWRm1pzONooA2KHo+SaSPtlfZROOmVkrOksLNp1g0DMizu+geMzMmiU6yUUuSV0iYlNLt44xM+twnSHBktw5djgwS9JDwH0ULZYdEQ8090Ezs0zkbDWtUvpg+wANJPfgahoPG7xzixYzs45TxotcaTfoTJIF/o+V1IfkhquDgHnACRGxoq31t7Rc4S7pCILngNnp45z08bm2HtDMrD2a1oRtbSvRvwDFU/8vBKZExD7AlPR1m7WUYGuBnum2Q9Hzps3MrONFiVsrJO1Oct+/G4uKx/DOYv+3kdxhpc1a6iJYFBGXtfC+mVnHKu9dZf8TuIB3D0XtHxGLACJikaRd2nOAllqw1bEkuJlZka3oIugnaWbRdsbbdUjHAksi4uksY22pBXtklgc2M2uT0luwyyJiRDPvfRj4rKRPkyxi1UvSHcBiSQPS1usAYEl7Qm22BRsRy9tTsZlZFtRY2taSiPheROye3jD1JODRiDgFeAgYm+42FniwPbFW+rbdZmalK28f7JZcAUyUdDowH3jv/eK3ghOsmeWGKP/FoYiYCkxNnzdQxu5RJ1gzy5dONpPLzKxqdLapsmZm1cMJ1swsA51wwW0zs+rhFqyZWTbcB2tmlhUn2PyJQiOFN96odBhVa8DUhkqHUPUKA/pUOoSqFg3lSTduwZqZZSEo64LbWXOCNbPc6DQ3PTQzq0pOsGZm2VDkJ8M6wZpZfmS/mlZZOcGaWa64D9bMLCOeKmtmlhW3YM3MMhDuIjAzy44TrJlZ+XmigZlZhtSYnwzrBGtm+eFxsGZm2fEwLTOzrLgFa2aWDV/kMjPLQgBe7MXMLBvugzUzy4DHwZqZZSXCXQRmZllxC9bMLCtOsGZm2XAL1swsCwEU8pNhnWDNLFfcgjUzy0qORhHUVDoAM7OtoShta7UeaQ9Jj0l6QdIcSf+SlveR9AdJL6ePO7U1VidYM8uP2IqtdZuAcyPiA8Bo4GxJQ4ALgSkRsQ8wJX3dJk6wZpYbAlSIkrbWRMSiiHgmfb4aeAEYCIwBbkt3uw04vq3xug/WzHJFpffB9pM0s+j1hIiYsMU6pUHAwcBTQP+IWARJEpa0S1tjdYI1s/zYujsaLIuIEa3tJKkn8CvgWxHxhqS2x7cZJ9gqM6L+Dcb9cCG1NcHv7+7DxGv7VzqkivvWuTMYOWoRK1d24xtnHAPAl8fOZvRhC2kMsWplN676yUiWN2xX4Ugr59vjpzNqxAJWrurOuHOOe7v8s5/5G5/9zIsUCjXMmDmQm24bXsEoy6G8axFIqiNJrndGxANp8WJJA9LW6wBgSVvr7/A+WEkFSbOKtkEt7Fsv6bAODK+iamqCs3/0OhefPJiv1+/Hx8asZM993qp0WBX3yOTBfP+iw99Vdv99+3P2mZ9k/LijmfHkAL50ypwKRVcd/jBlby7+wcffVTb0oH9w6KgFnHXOsZw5/jju/82QCkVXXmUcRSDgJuCFiLiq6K2HgLHp87HAg22NtRIt2HURMazEfeuBNcCfSq1cUm1EFNoQV8Xtd/CbLJzXlX/M7wbA1Ad35NBPrmL+y90rHFllPTd7Z3bpv/ZdZeverHv7effuBSLK97Muj557vj/9d1nzrrJjj3mJib86gI2bagFYtaqT/B2VrwX7YeDLwGxJs9Kyi4ArgImSTgfmA19o6wGqootA0jxgREQskzQCuBL4CjAOKEg6BRgPnA78v4i4P/3cmojoKakeuARYBAyTdBDJl1QPdAOui4jrO/Kc2qLvrhtZurDr26+XLapj/+FvVjCi6nbqabM58qh5rF1bx4Xn11c6nKozcLfVHDBkCWNPmcWGDbXceMtwXprbr9JhtU9Q0giBkqqKeJxkYMKWHFmOY1RimNZ2Rd0Dv25up4iYB/wCuDoihkXEtFbqHQn8a0QMIUnEqyLiEOAQ4OuSBpcp/sxsqW89R5NWOtzttxzE2JOPY+qje3HcmLmVDqfq1NY2skPPDXzr/GO48dbhXHTBNHK1FFVzyjcONnOVSLDr0oQ5LCL+qYz1zoiIV9PnRwOnps3+p4C+wD6bf0DSGZJmSpq5kfVlDKVtli2qY+fdNrz9ut+AjTT8o66FTxjA1Ef35MMfWVDpMKrOsoYePDF9D0C89HI/GhtF716V/ztvL0WUtFWDaplosIl3Ymmpo+jt/dIO6q5F7xV30gkYX5TIB0fE5M0ri4gJETEiIkbU0a19Z1AGL87qwcDBG+i/x3q61DVSP2YlT07uXemwqtJuA1e//XzUoQtZ8FqvCkZTnf701B58cOhiAAbu9gZ1dY2seqPyf+ft1nRXg9a2KlAVfbDAPOBDwO+Bfy4qXw302sJ+E0lmWzTXvJsEnCXp0YjYKGlf4PWIWNvM/lWhsSCu+9eB/OiuV6iphcn39OF/XuokFyba4YKLpjN06FJ69V7P7Xf9ljtuP4BDRi5i4O6riRBLFvfg2ms+VOkwK+rCc6cx9MDF9Oq1nl/e9AB33D2UyY+8j++Mn84v/uu3bNpUw5X/eRjNdznmRAA5uumhooMzfdOFqc3KPkoyXGIxyU/6ERFRnybG+0m+0vHASyRDJmpI5giPL7rIdV5EHJvWVwP8H+A4kr+opcDxEbGqubh6qU+MUln6tTul2gP2q3QIVa+xR9fWd9qGPfnc9byx9vV2Zfje2+8Wo4ecWdK+k2de+nQpEw2y1OEt2M2Ta1o2Ddh3C+UvAUM3Kx5d9Px76X5TgalFn2skGW5xUbsDNrPq0pifJmy1dBGYmbUuZ10ETrBmlivVMkKgFE6wZpYvTrBmZlmoniFYpXCCNbP88F1lzcyy4z5YM7OsOMGamWUggEYnWDOzDPgil5lZdpxgzcwyEEAhP1O5nGDNLEcCwgnWzCwb7iIwM8uARxGYmWXILVgzs4w4wZqZZSACCoVKR1EyJ1gzyxe3YM3MMuIEa2aWhfAoAjOzTASEJxqYmWXEU2XNzDIQ4dt2m5llxhe5zMyyEW7BmpllwQtum5llw4u9mJllI4DI0VTZmkoHYGZWskgX3C5la4WkYyS9KGmupAuzCNctWDPLlShDF4GkWuA64BPAAuDPkh6KiOfbXXkRt2DNLF/K04IdCcyNiFciYgNwDzCm3KEqcnRFLkuSlgL/U+k4ivQDllU6iCrm76d11fYd7RURO7enAkkPk5xXKboDbxW9nhARE9J6Pg8cExFfS19/GRgVEd9sT3ybcxdBqr3/4ctN0syIGFHpOKqVv5/WdcbvKCKOKVNV2lL1Zar7be4iMLNt0QJgj6LXuwMLy30QJ1gz2xb9GdhH0mBJXYGTgIfKfRB3EVSvCZUOoMr5+2mdv6NmRMQmSd8EJgG1wM0RMafcx/FFLjOzjLiLwMwsI06wZmYZcYKtAEkFSbMkPSfpPkk9tvLzgyR9Kav4stbe8y9TDPWSDuvo426tou+qaRvUwr65OKdtiRNsZayLiGERcSCwARi3lZ8fBOQ2wdLK+afTGLNWD+QhGTV9V03bvBb2rWcrz6mDvuttlhNs5U0D3i/pOElPSXpW0iOS+gNIOqKo9fKspB2AK4CPpmXfrmj07dd0/vWSHpN0FzBbUq2kn0j6s6S/SjoTQNIASX8sagF/NC0/WtJ0Sc+kreKeafk8ST9Iy2dL2j9tBY4Dvp3W89EKnXubpOfUL30+QtLULZ2TpFvTGUtNn1uTPpb0XVv7eZhWBUnqAnwKeBh4HBgdESHpa8AFwLnAecDZEfFEmjTeAi4EzouIYysUellsdv6QzA8/MCJelXQGsCoiDpHUDXhC0mTgc8CkiLg8bX31SJPNxcBREbFW0neB7wCXpfUui4jhkr5B8r19TdIvgDURcWXHnXGbbCdpVvr81Yj4py3tFBHzNj8nSae3UG+r33VEvFrG89gmOcFWRvE/mmnATcB+wL2SBgBdgaY/7ieAqyTdCTwQEQukLc3yy5Utnf9hwIyif9RHA0OLWmC9gX1IBojfLKkO+E1EzJJ0BDCEJDFA8v1NLzreA+nj0yQJOk/WRcSwDOot5bt2gm0nJ9jKeM8/Gkk/A66KiIck1QOXAkTEFZJ+B3waeFLSUR0baia2dP4Aa4uLgPERMWnzD0s6HPgM8EtJPwFWAH+IiC82c7z16WOBzvE3v4l3uve6l7Kfki+4a9F7JX3X1j7ug60evYHX0+djmwolvS8iZkfEfwAzgf2B1cAOHR9ih5oEnJW2VJG0r6TtJe0FLImIG0havsOBJ4EPS3p/um8PSfu2Un+ev8N5wIfS5/9cVL75ORXvNwaoa6a+LX7X5Qp2W+YEWz0uBe6TNI13LzH3rfRizl+AdcDvgb8CmyT9pRNc5GrOjcDzwDOSngOuJ2l91gOzJD1LklyuiYilwFeAuyX9lSTh7t9K/b8F/imPF7mAHwDXpH8rxfdP2fycbgCOkDQDGMW7W63FmvuurZ08VdbMLCNuwZqZZcQJ1swsI06wZmYZcYI1M8uIE6yZWUacYK0kKuMKWMVz5CXdKGlIC/u2aYWo4vn6pZRvts+arTzWpZLO29oYrfNzgrVSZbICVkR8LSKeb2GXevKx6pXZezjBWlts7QpYknStpOfTab+7NFWUrgQ1In1+jJJVr/4iaUozK0TtLOlX6TH+LOnD6Wf7SpqsZMWx69nybZnfRdJvJD0taU664Enxez9NY5kiaee07H2SHk4/M01Sa5MZbBvn2Rq2VdS2FbAOJlnM5iCgP8msoZs3q3dnkplHh6d19YmI5VtYIeou4OqIeFzSniTTPD8AXAI8HhGXSfoM8K6E2YyvpsfYDvizpF9FRAOwPfBMRJwr6d/Sur9JchPBcRHxsqRRwM+Bj7fha7RthBOslao9K2AdDtwdEQVgoaRHt1D/aOCPTXVFxPJm4jgKGKJ3VhTrpWSN3MNJV8qKiN9JWlHCOZ0jqWn5vz3SWBuARuDetPwO4AElS0UeRjKduenz3Uo4hm3DnGCtVG1eAUvSp4HW5mSrhH0g6dY6NCLWbSGWkud9K1mx7Ki0rjclTaX5lakiPe7KjJYOtE7KfbBWTs2tyvRH4KS0j3YA8LEtfHY6ycIkg9PP9knLN18hajLJz3XS/YalT/8InJyWfQrYqZVYewMr0uS6P0kLukkN0NQK/xJJ18MbwKuSvpAeQ5I+2MoxbBvnBGvl1NyqTL8GXgZmA/8X+O/NP5iuiHUGyc/xv/DOT/TNV4g6BxiRXkR7nndGM/wAOFzSMyRdFfNbifVhoIuS1bd+SLICV5O1wAGSnibpY226M8LJwOlpfHNIlgA0a5ZX0zIzy4hbsGZmGXGCNTPLiBOsmVlGnGDNzDLiBGtmlhEnWDOzjDjBmpll5H8BV95Gko2sQqoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_model(\"temporality\", model_checkpoints[model_type], epochs=20, lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5b5411d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning data preprocessing\n",
      "Processed data written to temporality_sentence_train_for_sequence_classification.json and temporality_sentence_dev_for_sequence_classification.json\n",
      "Train label distribution:\n",
      "Past: 570\n",
      "Present: 417\n",
      "Future: 107\n",
      "Dev label distribution:\n",
      "Past: 119\n",
      "Present: 48\n",
      "Future: 29\n",
      "Loading dataset into huggingface format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-91633e76452b96ad\n",
      "Reusing dataset json (/home/brentdevries/.cache/huggingface/datasets/json/default-91633e76452b96ad/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30c43e2748e44ffba1527e264318b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets using BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/brentdevries/.cache/huggingface/datasets/json/default-91633e76452b96ad/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-2f3eb69f7464c7bf.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845c01fc0a9144fabf372840f3b54144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT sequence classification model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running training *****\n",
      "  Num examples = 1094\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sent to cuda\n",
      "Setting up training configuration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1360' max='1360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1360/1360 14:05, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Micro Precision</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Micro Recall</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.257149</td>\n",
       "      <td>0.234694</td>\n",
       "      <td>0.234694</td>\n",
       "      <td>0.089147</td>\n",
       "      <td>0.234694</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.234694</td>\n",
       "      <td>0.139394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.160447</td>\n",
       "      <td>0.239796</td>\n",
       "      <td>0.239796</td>\n",
       "      <td>0.081174</td>\n",
       "      <td>0.239796</td>\n",
       "      <td>0.326389</td>\n",
       "      <td>0.239796</td>\n",
       "      <td>0.130014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.064276</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.387439</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.382528</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.252914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.016288</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.429563</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.511496</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.428756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.955500</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.452259</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.554972</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.481201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.850215</td>\n",
       "      <td>0.683673</td>\n",
       "      <td>0.683673</td>\n",
       "      <td>0.435516</td>\n",
       "      <td>0.683673</td>\n",
       "      <td>0.528653</td>\n",
       "      <td>0.683673</td>\n",
       "      <td>0.466811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.741780</td>\n",
       "      <td>0.709184</td>\n",
       "      <td>0.709184</td>\n",
       "      <td>0.455236</td>\n",
       "      <td>0.709184</td>\n",
       "      <td>0.550945</td>\n",
       "      <td>0.709184</td>\n",
       "      <td>0.484990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.989900</td>\n",
       "      <td>0.693257</td>\n",
       "      <td>0.719388</td>\n",
       "      <td>0.719388</td>\n",
       "      <td>0.453017</td>\n",
       "      <td>0.719388</td>\n",
       "      <td>0.548261</td>\n",
       "      <td>0.719388</td>\n",
       "      <td>0.487744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.989900</td>\n",
       "      <td>0.665070</td>\n",
       "      <td>0.729592</td>\n",
       "      <td>0.729592</td>\n",
       "      <td>0.466172</td>\n",
       "      <td>0.729592</td>\n",
       "      <td>0.566293</td>\n",
       "      <td>0.729592</td>\n",
       "      <td>0.499052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.989900</td>\n",
       "      <td>0.633895</td>\n",
       "      <td>0.739796</td>\n",
       "      <td>0.739796</td>\n",
       "      <td>0.797721</td>\n",
       "      <td>0.739796</td>\n",
       "      <td>0.572302</td>\n",
       "      <td>0.739796</td>\n",
       "      <td>0.522715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.989900</td>\n",
       "      <td>0.611912</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.695645</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.615884</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.608088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.989900</td>\n",
       "      <td>0.607645</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.745887</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.693136</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.695751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.989900</td>\n",
       "      <td>0.570820</td>\n",
       "      <td>0.801020</td>\n",
       "      <td>0.801020</td>\n",
       "      <td>0.760824</td>\n",
       "      <td>0.801020</td>\n",
       "      <td>0.710639</td>\n",
       "      <td>0.801020</td>\n",
       "      <td>0.721025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.989900</td>\n",
       "      <td>0.606621</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.754587</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.703288</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.709985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.434500</td>\n",
       "      <td>0.534300</td>\n",
       "      <td>0.826531</td>\n",
       "      <td>0.826531</td>\n",
       "      <td>0.761243</td>\n",
       "      <td>0.826531</td>\n",
       "      <td>0.742844</td>\n",
       "      <td>0.826531</td>\n",
       "      <td>0.751283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.434500</td>\n",
       "      <td>0.649940</td>\n",
       "      <td>0.801020</td>\n",
       "      <td>0.801020</td>\n",
       "      <td>0.758989</td>\n",
       "      <td>0.801020</td>\n",
       "      <td>0.732169</td>\n",
       "      <td>0.801020</td>\n",
       "      <td>0.735021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.434500</td>\n",
       "      <td>0.653224</td>\n",
       "      <td>0.811224</td>\n",
       "      <td>0.811224</td>\n",
       "      <td>0.739944</td>\n",
       "      <td>0.811224</td>\n",
       "      <td>0.729485</td>\n",
       "      <td>0.811224</td>\n",
       "      <td>0.731172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.434500</td>\n",
       "      <td>0.680092</td>\n",
       "      <td>0.790816</td>\n",
       "      <td>0.790816</td>\n",
       "      <td>0.716834</td>\n",
       "      <td>0.790816</td>\n",
       "      <td>0.722423</td>\n",
       "      <td>0.790816</td>\n",
       "      <td>0.715292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.434500</td>\n",
       "      <td>0.679088</td>\n",
       "      <td>0.811224</td>\n",
       "      <td>0.811224</td>\n",
       "      <td>0.737484</td>\n",
       "      <td>0.811224</td>\n",
       "      <td>0.777907</td>\n",
       "      <td>0.811224</td>\n",
       "      <td>0.752881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.434500</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.710498</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.715073</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.705696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to cmed-temporality/checkpoint-500\n",
      "Configuration saved in cmed-temporality/checkpoint-500/config.json\n",
      "Model weights saved in cmed-temporality/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-temporality/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in cmed-temporality/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-temporality/checkpoint-1000\n",
      "Configuration saved in cmed-temporality/checkpoint-1000/config.json\n",
      "Model weights saved in cmed-temporality/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-temporality/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in cmed-temporality/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 196\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_model(\"temporality\", model_checkpoints[model_type], epochs=20, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad749e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
