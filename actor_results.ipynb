{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7243dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_model\n",
    "from run_model import run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ee2f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"domain\" # choose checkpoint type\n",
    "model_checkpoints = {\n",
    "    \"fast\": \"distilbert-base-uncased\",\n",
    "    \"base\": \"bert-base-uncased\",\n",
    "    \"domain\": \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f86c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fef98176ee980796\n",
      "Reusing dataset json (/home/brentdevries/.cache/huggingface/datasets/json/default-fef98176ee980796/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning data preprocessing\n",
      "Processed data written to actor_tokens200_train_for_token_classification.json and actor_tokens200_dev_for_token_classification.json\n",
      "Train label distribution:\n",
      "Physician: 1017\n",
      "Patient: 84\n",
      "Dev label distribution:\n",
      "Physician: 174\n",
      "Patient: 16\n",
      "Loading dataset into huggingface format\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a72fa91ea8c4f2da69ffed7a8460682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets using BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/config.json from cache at /home/brentdevries/.cache/huggingface/transformers/dc6d60ebe42d83e1479ce0d473758bb3586763ff6c4c814bda5321acf856bd64.b74d0770929e519c6d193d16b6874051ae549f5c8c228903a48e59d36260466b\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/vocab.txt from cache at /home/brentdevries/.cache/huggingface/transformers/d544709b8432c5d7b9fef7d9361a9cc048632d93776ae61e401a6eff0fb8f037.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/config.json from cache at /home/brentdevries/.cache/huggingface/transformers/dc6d60ebe42d83e1479ce0d473758bb3586763ff6c4c814bda5321acf856bd64.b74d0770929e519c6d193d16b6874051ae549f5c8c228903a48e59d36260466b\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/config.json from cache at /home/brentdevries/.cache/huggingface/transformers/dc6d60ebe42d83e1479ce0d473758bb3586763ff6c4c814bda5321acf856bd64.b74d0770929e519c6d193d16b6874051ae549f5c8c228903a48e59d36260466b\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/brentdevries/.cache/huggingface/datasets/json/default-fef98176ee980796/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-19e4ebf5ff9b4476.arrow\n",
      "Loading cached processed dataset at /home/brentdevries/.cache/huggingface/datasets/json/default-fef98176ee980796/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-ac7ec44075ecf94d.arrow\n",
      "loading configuration file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/config.json from cache at /home/brentdevries/.cache/huggingface/transformers/dc6d60ebe42d83e1479ce0d473758bb3586763ff6c4c814bda5321acf856bd64.b74d0770929e519c6d193d16b6874051ae549f5c8c228903a48e59d36260466b\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"attention_probs_dropout_prob\": 0.2,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.2,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT token classification model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT/resolve/main/pytorch_model.bin from cache at /home/brentdevries/.cache/huggingface/transformers/794538e7c825dc7be96d9fc3c73b79a9736da5f699fc50d31513dbca0740b349.f0d8b668347b3048f5b88e273fde3c3412366726bc99aa5935b7990944092fb1\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running training *****\n",
      "  Num examples = 534\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 3300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sent to cuda\n",
      "Setting up training configuration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3300' max='3300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3300/3300 1:04:27, Epoch 99/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Micro Precision</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Micro Recall</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.738347</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.487987</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.470187</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.267373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.737024</td>\n",
       "      <td>0.294737</td>\n",
       "      <td>0.294737</td>\n",
       "      <td>0.489286</td>\n",
       "      <td>0.294737</td>\n",
       "      <td>0.473060</td>\n",
       "      <td>0.294737</td>\n",
       "      <td>0.271406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.734066</td>\n",
       "      <td>0.321053</td>\n",
       "      <td>0.321053</td>\n",
       "      <td>0.495286</td>\n",
       "      <td>0.321053</td>\n",
       "      <td>0.487428</td>\n",
       "      <td>0.321053</td>\n",
       "      <td>0.291188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.730157</td>\n",
       "      <td>0.336842</td>\n",
       "      <td>0.336842</td>\n",
       "      <td>0.498563</td>\n",
       "      <td>0.336842</td>\n",
       "      <td>0.496049</td>\n",
       "      <td>0.336842</td>\n",
       "      <td>0.302773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.724852</td>\n",
       "      <td>0.384211</td>\n",
       "      <td>0.384211</td>\n",
       "      <td>0.497844</td>\n",
       "      <td>0.384211</td>\n",
       "      <td>0.493534</td>\n",
       "      <td>0.384211</td>\n",
       "      <td>0.332252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.719452</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.495307</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.485273</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.352059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.713692</td>\n",
       "      <td>0.478947</td>\n",
       "      <td>0.478947</td>\n",
       "      <td>0.487695</td>\n",
       "      <td>0.478947</td>\n",
       "      <td>0.460129</td>\n",
       "      <td>0.478947</td>\n",
       "      <td>0.376554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.708738</td>\n",
       "      <td>0.505263</td>\n",
       "      <td>0.505263</td>\n",
       "      <td>0.492125</td>\n",
       "      <td>0.505263</td>\n",
       "      <td>0.474497</td>\n",
       "      <td>0.505263</td>\n",
       "      <td>0.392021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.702833</td>\n",
       "      <td>0.563158</td>\n",
       "      <td>0.563158</td>\n",
       "      <td>0.483795</td>\n",
       "      <td>0.563158</td>\n",
       "      <td>0.449353</td>\n",
       "      <td>0.563158</td>\n",
       "      <td>0.409164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.697376</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.471077</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.415589</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.411619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693401</td>\n",
       "      <td>0.668421</td>\n",
       "      <td>0.668421</td>\n",
       "      <td>0.480857</td>\n",
       "      <td>0.668421</td>\n",
       "      <td>0.450072</td>\n",
       "      <td>0.668421</td>\n",
       "      <td>0.442192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.688574</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>0.476512</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>0.447557</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>0.450632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.685014</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.465188</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.430675</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.443011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.682177</td>\n",
       "      <td>0.752632</td>\n",
       "      <td>0.752632</td>\n",
       "      <td>0.449686</td>\n",
       "      <td>0.752632</td>\n",
       "      <td>0.410920</td>\n",
       "      <td>0.752632</td>\n",
       "      <td>0.429429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>0.773684</td>\n",
       "      <td>0.773684</td>\n",
       "      <td>0.450920</td>\n",
       "      <td>0.773684</td>\n",
       "      <td>0.422414</td>\n",
       "      <td>0.773684</td>\n",
       "      <td>0.436202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.677266</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.451807</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.431034</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.441176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.675017</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.451807</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.431034</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.441176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.674407</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.436782</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.671805</td>\n",
       "      <td>0.836842</td>\n",
       "      <td>0.836842</td>\n",
       "      <td>0.454286</td>\n",
       "      <td>0.836842</td>\n",
       "      <td>0.456897</td>\n",
       "      <td>0.836842</td>\n",
       "      <td>0.455587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.670883</td>\n",
       "      <td>0.836842</td>\n",
       "      <td>0.836842</td>\n",
       "      <td>0.454286</td>\n",
       "      <td>0.836842</td>\n",
       "      <td>0.456897</td>\n",
       "      <td>0.836842</td>\n",
       "      <td>0.455587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.669181</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.454802</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.462644</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.458689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.667826</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.456044</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.477011</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.466292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.666907</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.469274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.666335</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.469274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.665219</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.471264</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.463277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.664622</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.455056</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.460227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.663429</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.455056</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.460227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.661884</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.455556</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.471264</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.463277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.660573</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.455056</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.460227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.660356</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.454802</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.462644</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.458689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.659203</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.454802</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.462644</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.458689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.660665</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.454802</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.462644</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.458689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.658749</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.453757</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.451149</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.452450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.666337</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.459770</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.457143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.658434</td>\n",
       "      <td>0.810526</td>\n",
       "      <td>0.810526</td>\n",
       "      <td>0.452941</td>\n",
       "      <td>0.810526</td>\n",
       "      <td>0.442529</td>\n",
       "      <td>0.810526</td>\n",
       "      <td>0.447674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.663639</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.453216</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.445402</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.449275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.671617</td>\n",
       "      <td>0.821053</td>\n",
       "      <td>0.821053</td>\n",
       "      <td>0.453488</td>\n",
       "      <td>0.821053</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.821053</td>\n",
       "      <td>0.450867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.660646</td>\n",
       "      <td>0.794737</td>\n",
       "      <td>0.794737</td>\n",
       "      <td>0.494092</td>\n",
       "      <td>0.794737</td>\n",
       "      <td>0.490661</td>\n",
       "      <td>0.794737</td>\n",
       "      <td>0.488648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.664885</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.513448</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.521911</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.511634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.667804</td>\n",
       "      <td>0.805263</td>\n",
       "      <td>0.805263</td>\n",
       "      <td>0.531698</td>\n",
       "      <td>0.805263</td>\n",
       "      <td>0.553161</td>\n",
       "      <td>0.805263</td>\n",
       "      <td>0.533665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.694875</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.495779</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.493534</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.491406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.684557</td>\n",
       "      <td>0.794737</td>\n",
       "      <td>0.794737</td>\n",
       "      <td>0.526780</td>\n",
       "      <td>0.794737</td>\n",
       "      <td>0.547414</td>\n",
       "      <td>0.794737</td>\n",
       "      <td>0.526548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.711025</td>\n",
       "      <td>0.805263</td>\n",
       "      <td>0.805263</td>\n",
       "      <td>0.531698</td>\n",
       "      <td>0.805263</td>\n",
       "      <td>0.553161</td>\n",
       "      <td>0.805263</td>\n",
       "      <td>0.533665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.701183</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.550866</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.601293</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.554409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.716749</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.543315</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.578664</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.546938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>0.698944</td>\n",
       "      <td>0.757895</td>\n",
       "      <td>0.757895</td>\n",
       "      <td>0.548724</td>\n",
       "      <td>0.757895</td>\n",
       "      <td>0.612428</td>\n",
       "      <td>0.757895</td>\n",
       "      <td>0.544792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>0.730799</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.577398</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.644037</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.589987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>0.765848</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.572392</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.621408</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.584080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>0.751366</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.584601</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.649784</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.599387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>0.768391</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.584601</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.649784</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.599387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>0.794390</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.584601</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.649784</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.599387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>0.830872</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.594880</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.635776</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.608824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>0.837036</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.612727</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.667026</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.630909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>0.903304</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.570175</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.581897</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.575155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>1.024535</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.615267</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.621767</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.618374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>0.900987</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.607176</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.664152</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.625247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>1.012366</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.592647</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.613147</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.601098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>1.063648</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.592647</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.613147</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.601098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>1.105079</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.592647</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.613147</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.601098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.386300</td>\n",
       "      <td>1.082893</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.613271</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.644397</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.625719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>1.171083</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.615267</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.621767</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.618374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>1.307363</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>1.257036</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>1.294108</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>1.407317</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.599048</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.593391</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.596081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>1.402000</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>1.376806</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.668282</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.687141</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.676981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>1.741588</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.578661</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.565014</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.570685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>1.492646</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>1.558569</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>1.554570</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>1.561053</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>1.687231</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>1.581684</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.668282</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.687141</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.676981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>1.622144</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.647569</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.655891</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.651559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>1.591435</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.668282</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.687141</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.676981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>1.910823</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.600051</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.570761</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.581366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>1.696545</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>1.650309</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.668282</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.687141</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.676981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>1.748827</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>1.877165</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.635238</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.627514</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.631204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>2.017840</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.613889</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.573635</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.587136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>1.821094</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>2.007985</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.632959</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.602011</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.614448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>1.823584</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.647569</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.655891</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.651559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>1.932526</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>2.249459</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.651786</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.579382</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.599719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>2.011123</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.635238</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.627514</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.631204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>1.821276</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.668282</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.687141</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.676981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>2.077622</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.677434</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.636135</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.653003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>2.022995</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>2.071025</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.677434</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.636135</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.653003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>2.293425</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.651786</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.579382</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.599719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>2.200873</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.607759</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.628422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>2.096277</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.661234</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.633261</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.645348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>2.208079</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.607759</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.628422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>2.005013</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.647569</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.655891</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.651559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>2.052936</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.647569</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.655891</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.651559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>2.211319</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.677434</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.636135</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.653003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>2.084176</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.624641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-actor/checkpoint-500\n",
      "Configuration saved in cmed-actor/checkpoint-500/config.json\n",
      "Model weights saved in cmed-actor/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-actor/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in cmed-actor/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-actor/checkpoint-1000\n",
      "Configuration saved in cmed-actor/checkpoint-1000/config.json\n",
      "Model weights saved in cmed-actor/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-actor/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in cmed-actor/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-actor/checkpoint-1500\n",
      "Configuration saved in cmed-actor/checkpoint-1500/config.json\n",
      "Model weights saved in cmed-actor/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-actor/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in cmed-actor/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-actor/checkpoint-2000\n",
      "Configuration saved in cmed-actor/checkpoint-2000/config.json\n",
      "Model weights saved in cmed-actor/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-actor/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in cmed-actor/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-actor/checkpoint-2500\n",
      "Configuration saved in cmed-actor/checkpoint-2500/config.json\n",
      "Model weights saved in cmed-actor/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-actor/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in cmed-actor/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-actor/checkpoint-3000\n",
      "Configuration saved in cmed-actor/checkpoint-3000/config.json\n",
      "Model weights saved in cmed-actor/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-actor/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in cmed-actor/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: tokens, actor_tags.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 86\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_model(\"actor\", model_checkpoints[model_type], epochs=100, classification=\"token\", chunk_by=\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ee5abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning data preprocessing\n",
      "Processed data written to actor_sentence_train_for_sequence_classification.json and actor_sentence_dev_for_sequence_classification.json\n",
      "Train label distribution:\n",
      "Physician: 1017\n",
      "Patient: 84\n",
      "Dev label distribution:\n",
      "Physician: 174\n",
      "Patient: 16\n",
      "Loading dataset into huggingface format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fcbec3294461ebbf\n",
      "Reusing dataset json (/home/brentdevries/.cache/huggingface/datasets/json/default-fcbec3294461ebbf/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447173ede3894c92b7d5e88da76a4f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets using BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/brentdevries/.cache/huggingface/datasets/json/default-fcbec3294461ebbf/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-93c23e973bad42c5.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2efaec8e9940ba94148bc17660f01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT sequence classification model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running training *****\n",
      "  Num examples = 1101\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sent to cuda\n",
      "Setting up training configuration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1380 15:05, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Micro Precision</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Micro Recall</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.581188</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.457895</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.478022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.578014</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.457895</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.478022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.573898</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.457895</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.478022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.567412</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.457895</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.478022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.556032</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.457895</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.478022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.541403</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.457895</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.478022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.528265</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.457895</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.478022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.599800</td>\n",
       "      <td>0.507893</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.710106</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.528376</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.533456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.599800</td>\n",
       "      <td>0.491017</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.619948</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.599138</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.608017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.599800</td>\n",
       "      <td>0.501203</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.606602</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.641523</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.619883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.599800</td>\n",
       "      <td>0.517880</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.629456</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.698276</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.652015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.599800</td>\n",
       "      <td>0.593281</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.629456</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.698276</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.652015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.599800</td>\n",
       "      <td>0.676090</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.613086</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.634494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.599800</td>\n",
       "      <td>0.846052</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.592647</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.613147</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.601098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.332400</td>\n",
       "      <td>0.891414</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.618166</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.692529</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.640152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.332400</td>\n",
       "      <td>1.041634</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.599415</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.616020</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.606625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.332400</td>\n",
       "      <td>1.177462</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.630755</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.576509</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.593239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.332400</td>\n",
       "      <td>1.265880</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.606602</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.641523</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.619883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.332400</td>\n",
       "      <td>1.267752</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.620588</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.647270</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.631783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.332400</td>\n",
       "      <td>1.339051</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.620588</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.647270</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.631783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to cmed-actor/checkpoint-500\n",
      "Configuration saved in cmed-actor/checkpoint-500/config.json\n",
      "Model weights saved in cmed-actor/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-actor/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in cmed-actor/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-actor/checkpoint-1000\n",
      "Configuration saved in cmed-actor/checkpoint-1000/config.json\n",
      "Model weights saved in cmed-actor/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-actor/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in cmed-actor/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_model(\"actor\", model_checkpoints[model_type], epochs=20, lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb165e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fcbec3294461ebbf\n",
      "Reusing dataset json (/home/brentdevries/.cache/huggingface/datasets/json/default-fcbec3294461ebbf/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning data preprocessing\n",
      "Processed data written to actor_sentence_train_for_sequence_classification.json and actor_sentence_dev_for_sequence_classification.json\n",
      "Train label distribution:\n",
      "Physician: 1017\n",
      "Patient: 84\n",
      "Dev label distribution:\n",
      "Physician: 174\n",
      "Patient: 16\n",
      "Loading dataset into huggingface format\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04b2c19ac9a4e6d802eb917b5a943a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets using BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/brentdevries/.cache/huggingface/datasets/json/default-fcbec3294461ebbf/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-93c23e973bad42c5.arrow\n",
      "Loading cached processed dataset at /home/brentdevries/.cache/huggingface/datasets/json/default-fcbec3294461ebbf/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-09ee9fe509b2cdab.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT sequence classification model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running training *****\n",
      "  Num examples = 1101\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sent to cuda\n",
      "Setting up training configuration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1380 15:15, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Micro Precision</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Micro Recall</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.623744</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.493101</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.493894</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.493333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.603872</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.521291</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.511135</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.510768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.584283</td>\n",
       "      <td>0.910526</td>\n",
       "      <td>0.910526</td>\n",
       "      <td>0.457672</td>\n",
       "      <td>0.910526</td>\n",
       "      <td>0.497126</td>\n",
       "      <td>0.910526</td>\n",
       "      <td>0.476584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.572974</td>\n",
       "      <td>0.910526</td>\n",
       "      <td>0.910526</td>\n",
       "      <td>0.457672</td>\n",
       "      <td>0.910526</td>\n",
       "      <td>0.497126</td>\n",
       "      <td>0.910526</td>\n",
       "      <td>0.476584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.564727</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.457895</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>0.478022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.554954</td>\n",
       "      <td>0.910526</td>\n",
       "      <td>0.910526</td>\n",
       "      <td>0.662162</td>\n",
       "      <td>0.910526</td>\n",
       "      <td>0.553879</td>\n",
       "      <td>0.910526</td>\n",
       "      <td>0.571561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.541914</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.592647</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.613147</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.601098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.610200</td>\n",
       "      <td>0.520005</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.599415</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.616020</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.606625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.610200</td>\n",
       "      <td>0.520734</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.608766</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.596264</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.601905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.610200</td>\n",
       "      <td>0.553637</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.608766</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.596264</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.601905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.610200</td>\n",
       "      <td>0.588084</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.608766</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.596264</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.601905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.610200</td>\n",
       "      <td>0.678796</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.619948</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.599138</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.608017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.610200</td>\n",
       "      <td>0.741870</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.668282</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.687141</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.676981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.610200</td>\n",
       "      <td>0.835600</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.679871</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.690014</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.684744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>0.909971</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.698966</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.721264</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.709283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>1.041933</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.632959</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.602011</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.614448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>1.062705</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.648299</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.604885</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.621236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>1.083563</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.635238</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.627514</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.631204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>1.185907</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.647321</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.630388</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.638095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>1.203164</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.619948</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.599138</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.608017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-actor/checkpoint-500\n",
      "Configuration saved in cmed-actor/checkpoint-500/config.json\n",
      "Model weights saved in cmed-actor/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-actor/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in cmed-actor/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to cmed-actor/checkpoint-1000\n",
      "Configuration saved in cmed-actor/checkpoint-1000/config.json\n",
      "Model weights saved in cmed-actor/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in cmed-actor/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in cmed-actor/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: tokens.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 190\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_model(\"actor\", model_checkpoints[model_type], epochs=20, lr=2e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
